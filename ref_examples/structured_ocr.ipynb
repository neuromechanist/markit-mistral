{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7FPiAIwHteCl"
      },
      "source": [
        "# OCR Cookbook\n",
        "\n",
        "---\n",
        "\n",
        "## OCR Exploration and Simple Structured Outputs (Deprecated)\n",
        "In this cookbook, we will explore the basics of OCR and leverage it together with existing models to achieve structured outputs fueled by our OCR model (we recommend using the new Annotations feature instead for better results).\n",
        "\n",
        "You may want to do this in case current vision models are not powerful enough, hence enhancing their vision OCR capabilities with the OCR model to achieve better structured data extraction.\n",
        "\n",
        "---\n",
        "\n",
        "### Model Used\n",
        "- Mistral OCR\n",
        "- Pixtral 12B & Ministral 8B\n",
        "\n",
        "---\n",
        "\n",
        "**For a more up to date guide on structured outputs visit our [Annotations cookbook](https://github.com/mistralai/cookbook/blob/main/mistral/ocr/data_extraction.ipynb) on Data Extraction.**\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UgZW4ZfetwAl"
      },
      "source": [
        "## Setup\n",
        "\n",
        "First, let's install `mistralai` and download the required files."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "po7Cukllt8za"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!pip install mistralai"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8rxv4Tx5kNX"
      },
      "source": [
        "### Download PDF and image files"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MtKgrASwF3Ol"
      },
      "outputs": [],
      "source": [
        "%%capture\n",
        "!wget https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/mistral7b.pdf\n",
        "!wget https://raw.githubusercontent.com/mistralai/cookbook/refs/heads/main/mistral/ocr/receipt.png"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NhwM0aITt7ti"
      },
      "source": [
        "## Mistral OCR with PDF\n",
        "\n",
        "We will need to set up our client. You can create an API key on our [Plateforme](https://console.mistral.ai/api-keys/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "odfkuCk6qSAw"
      },
      "outputs": [],
      "source": [
        "# Initialize Mistral client with API key\n",
        "from mistralai import Mistral\n",
        "\n",
        "api_key = \"API_KEY\" # Replace with your API key\n",
        "client = Mistral(api_key=api_key)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xk-3YwljuFKK"
      },
      "source": [
        "There are two types of files you can apply OCR to:\n",
        "- 1. PDF files\n",
        "- 2. Image files\n",
        "\n",
        "Let's start with a PDF file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "svaJGBFlqm7_",
        "outputId": "d59c5b68-486c-41a3-a0b9-8f6c0baa14d3"
      },
      "outputs": [],
      "source": [
        "# Import required libraries\n",
        "from pathlib import Path\n",
        "from mistralai import DocumentURLChunk, ImageURLChunk, TextChunk\n",
        "import json\n",
        "\n",
        "# Verify PDF file exists\n",
        "pdf_file = Path(\"mistral7b.pdf\")\n",
        "assert pdf_file.is_file()\n",
        "\n",
        "# Upload PDF file to Mistral's OCR service\n",
        "uploaded_file = client.files.upload(\n",
        "    file={\n",
        "        \"file_name\": pdf_file.stem,\n",
        "        \"content\": pdf_file.read_bytes(),\n",
        "    },\n",
        "    purpose=\"ocr\",\n",
        ")\n",
        "\n",
        "# Get URL for the uploaded file\n",
        "signed_url = client.files.get_signed_url(file_id=uploaded_file.id, expiry=1)\n",
        "\n",
        "# Process PDF with OCR, including embedded images\n",
        "pdf_response = client.ocr.process(\n",
        "    document=DocumentURLChunk(document_url=signed_url.url),\n",
        "    model=\"mistral-ocr-latest\",\n",
        "    include_image_base64=True\n",
        ")\n",
        "\n",
        "# Convert response to JSON format\n",
        "response_dict = json.loads(pdf_response.model_dump_json())\n",
        "\n",
        "print(json.dumps(response_dict, indent=4)[0:1000]) # check the first 1000 characters"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EG2_TdlKIxYs"
      },
      "source": [
        "View the result with the following:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "dxefUpm-Idp8",
        "outputId": "715b1d4b-5afb-4b96-e15d-ec3fc1b287b5"
      },
      "outputs": [],
      "source": [
        "from mistralai.models import OCRResponse\n",
        "from IPython.display import Markdown, display\n",
        "\n",
        "def replace_images_in_markdown(markdown_str: str, images_dict: dict) -> str:\n",
        "    \"\"\"\n",
        "    Replace image placeholders in markdown with base64-encoded images.\n",
        "\n",
        "    Args:\n",
        "        markdown_str: Markdown text containing image placeholders\n",
        "        images_dict: Dictionary mapping image IDs to base64 strings\n",
        "\n",
        "    Returns:\n",
        "        Markdown text with images replaced by base64 data\n",
        "    \"\"\"\n",
        "    for img_name, base64_str in images_dict.items():\n",
        "        markdown_str = markdown_str.replace(\n",
        "            f\"![{img_name}]({img_name})\", f\"![{img_name}]({base64_str})\"\n",
        "        )\n",
        "    return markdown_str\n",
        "\n",
        "def get_combined_markdown(ocr_response: OCRResponse) -> str:\n",
        "    \"\"\"\n",
        "    Combine OCR text and images into a single markdown document.\n",
        "\n",
        "    Args:\n",
        "        ocr_response: Response from OCR processing containing text and images\n",
        "\n",
        "    Returns:\n",
        "        Combined markdown string with embedded images\n",
        "    \"\"\"\n",
        "    markdowns: list[str] = []\n",
        "    # Extract images from page\n",
        "    for page in ocr_response.pages:\n",
        "        image_data = {}\n",
        "        for img in page.images:\n",
        "            image_data[img.id] = img.image_base64\n",
        "        # Replace image placeholders with actual images\n",
        "        markdowns.append(replace_images_in_markdown(page.markdown, image_data))\n",
        "\n",
        "    return \"\\n\\n\".join(markdowns)\n",
        "\n",
        "# Display combined markdowns and images\n",
        "display(Markdown(get_combined_markdown(pdf_response)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8yk5tBpPuKal"
      },
      "source": [
        "## Mistral OCR with Image\n",
        "\n",
        "In addition to the PDF file shown above, Mistral OCR can also process image files:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sFdyKIcgrahm",
        "outputId": "c1bf6808-a39c-49d0-a9a9-9d68be9a172b"
      },
      "outputs": [],
      "source": [
        "import base64\n",
        "\n",
        "# Verify image exists\n",
        "image_file = Path(\"receipt.png\")\n",
        "assert image_file.is_file()\n",
        "\n",
        "# Encode image as base64 for API\n",
        "encoded = base64.b64encode(image_file.read_bytes()).decode()\n",
        "base64_data_url = f\"data:image/jpeg;base64,{encoded}\"\n",
        "\n",
        "# Process image with OCR\n",
        "image_response = client.ocr.process(\n",
        "    document=ImageURLChunk(image_url=base64_data_url),\n",
        "    model=\"mistral-ocr-latest\"\n",
        ")\n",
        "\n",
        "# Convert response to JSON\n",
        "response_dict = json.loads(image_response.model_dump_json())\n",
        "json_string = json.dumps(response_dict, indent=4)\n",
        "print(json_string)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pWStbt7LuMvT"
      },
      "source": [
        "## Extract structured data from OCR results\n",
        "\n",
        "OCR results can be further processed using another model.\n",
        "\n",
        "Our goal is to extract structured data from these results. To achieve this, we will utilize the `pixtral-12b-latest` model, supported by our OCR model, to deliver better and higher-quality answers:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8aZOQs38r0GO",
        "outputId": "3d406569-57bc-4efb-cc92-e9a670f97ed7"
      },
      "outputs": [],
      "source": [
        "# Get OCR results for processing\n",
        "image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "# Get structured response from model\n",
        "chat_response = client.chat.complete(\n",
        "    model=\"pixtral-12b-latest\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                ImageURLChunk(image_url=base64_data_url),\n",
        "                TextChunk(\n",
        "                    text=(\n",
        "                        f\"This is image's OCR in markdown:\\n\\n{image_ocr_markdown}\\n.\\n\"\n",
        "                        \"Convert this into a sensible structured json response. \"\n",
        "                        \"The output should be strictly be json with no extra commentary\"\n",
        "                    )\n",
        "                ),\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# Parse and return JSON response\n",
        "response_dict = json.loads(chat_response.choices[0].message.content)\n",
        "print(json.dumps(response_dict, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4YKioib1vgTZ"
      },
      "source": [
        "In the example above, we are leveraging a model already capable of vision tasks.\n",
        "\n",
        "However, we could also use text-only models for the structured output. Note in this case, we do not include the image in the user message:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1m19STu2DDfI",
        "outputId": "06f99dfe-b697-4d82-bf20-0fa60435d47f"
      },
      "outputs": [],
      "source": [
        "# Get OCR results for processing\n",
        "image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "# Get structured response from model\n",
        "chat_response = client.chat.complete(\n",
        "    model=\"ministral-8b-latest\",\n",
        "    messages=[\n",
        "        {\n",
        "            \"role\": \"user\",\n",
        "            \"content\": [\n",
        "                TextChunk(\n",
        "                    text=(\n",
        "                        f\"This is image's OCR in markdown:\\n\\n{image_ocr_markdown}\\n.\\n\"\n",
        "                        \"Convert this into a sensible structured json response. \"\n",
        "                        \"The output should be strictly be json with no extra commentary\"\n",
        "                    )\n",
        "                ),\n",
        "            ],\n",
        "        }\n",
        "    ],\n",
        "    response_format={\"type\": \"json_object\"},\n",
        "    temperature=0,\n",
        ")\n",
        "\n",
        "# Parse and return JSON response\n",
        "response_dict = json.loads(chat_response.choices[0].message.content)\n",
        "print(json.dumps(response_dict, indent=4))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Pc__PKmkwUnZ"
      },
      "source": [
        "## All Together - Mistral OCR + Custom Structured Output\n",
        "Let's design a simple function that takes an `image_path` file and returns a JSON structured output in a specific format. In this case, we arbitrarily decided we wanted an output respecting the following:\n",
        "\n",
        "```python\n",
        "class StructuredOCR:\n",
        "    file_name: str  # can be any string\n",
        "    topics: list[str]  # must be a list of strings\n",
        "    languages: str  # string\n",
        "    ocr_contents: dict  # any dictionary, can be freely defined by the model\n",
        "```\n",
        "\n",
        "We will make use of [custom structured outputs](https://docs.mistral.ai/capabilities/structured-output/custom_structured_output/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oM2ensmIwh4H"
      },
      "outputs": [],
      "source": [
        "from enum import Enum\n",
        "from pathlib import Path\n",
        "from pydantic import BaseModel\n",
        "import base64\n",
        "\n",
        "\n",
        "class StructuredOCR(BaseModel):\n",
        "    file_name: str\n",
        "    topics: list[str]\n",
        "    languages: str\n",
        "    ocr_contents: dict\n",
        "\n",
        "def structured_ocr(image_path: str) -> StructuredOCR:\n",
        "    \"\"\"\n",
        "    Process an image using OCR and extract structured data.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to the image file to process\n",
        "\n",
        "    Returns:\n",
        "        StructuredOCR object containing the extracted data\n",
        "\n",
        "    Raises:\n",
        "        AssertionError: If the image file does not exist\n",
        "    \"\"\"\n",
        "    # Validate input file\n",
        "    image_file = Path(image_path)\n",
        "    assert image_file.is_file(), \"The provided image path does not exist.\"\n",
        "\n",
        "    # Read and encode the image file\n",
        "    encoded_image = base64.b64encode(image_file.read_bytes()).decode()\n",
        "    base64_data_url = f\"data:image/jpeg;base64,{encoded_image}\"\n",
        "\n",
        "    # Process the image using OCR\n",
        "    image_response = client.ocr.process(\n",
        "        document=ImageURLChunk(image_url=base64_data_url),\n",
        "        model=\"mistral-ocr-latest\"\n",
        "    )\n",
        "    image_ocr_markdown = image_response.pages[0].markdown\n",
        "\n",
        "    # Parse the OCR result into a structured JSON response\n",
        "    chat_response = client.chat.parse(\n",
        "        model=\"pixtral-12b-latest\",\n",
        "        messages=[\n",
        "            {\n",
        "                \"role\": \"user\",\n",
        "                \"content\": [\n",
        "                    ImageURLChunk(image_url=base64_data_url),\n",
        "                    TextChunk(text=(\n",
        "                        f\"This is the image's OCR in markdown:\\n{image_ocr_markdown}\\n.\\n\"\n",
        "                        \"Convert this into a structured JSON response \"\n",
        "                        \"with the OCR contents in a sensible dictionnary.\"\n",
        "                        )\n",
        "                    )\n",
        "                ]\n",
        "            }\n",
        "        ],\n",
        "        response_format=StructuredOCR,\n",
        "        temperature=0\n",
        "    )\n",
        "\n",
        "    return chat_response.choices[0].message.parsed"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UVipACEOAyEX"
      },
      "source": [
        "We can now extract structured output from any image parsed with our OCR model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uvt3OAcpyXCF",
        "outputId": "3d22bc4d-005f-4e59-e974-bb871f4882d8"
      },
      "outputs": [],
      "source": [
        "# Example usage\n",
        "image_path = \"receipt.png\" # Path to sample receipt image\n",
        "structured_response = structured_ocr(image_path) # Process image and extract data\n",
        "\n",
        "# Parse and return JSON response\n",
        "response_dict = json.loads(structured_response.model_dump_json())\n",
        "print(json.dumps(response_dict, indent=4))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H8705WaqA8KV"
      },
      "source": [
        "The original image for comparison can be found below."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "1Xj9tOTKA7mw",
        "outputId": "8933d2be-3b06-41f6-be7a-4c396830a57c"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "\n",
        "image = Image.open(image_path)\n",
        "image.resize((image.width // 5, image.height // 5))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yC4udP5Bpe72"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
